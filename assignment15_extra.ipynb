{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 15 (EXTRA) - Contrastive Learning\n",
    "\n",
    "This assignment is for students that need to an extra assignment to pass the tutorial part of the module. It is not mandatory for students that have already passed the tutorial part, but might be good practice anyway. \n",
    "\n",
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf.\n",
    "\n",
    "#### Please state both names of your group members here:\n",
    "Jane and John Doe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15.1: Baseline\n",
    "\n",
    "Implement a small supervised \"Projection Head\" (1-layer fully connected model) that is trained on [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) input samples directly. We want to have a baseline accuracy to compare against later. This helps to evaluate the representation quality. Try using similar hyperparameters (i.e., learning rate) for your contrastive learning model in 15.1.2 and 15.1.3. <br><br>\n",
    "* Report the number of parameters and accuracy of your 1-layer FC model. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15.2: Contrastive Loss Function\n",
    "\n",
    "- Implement a ConvNet to learn representations in a constrastive fashion for MNIST data. 3 Conv layers should be sufficient. You don't need a fully connected layer in the end during training. **(RESULT)**\n",
    "- Test the quality of your representations using a classifier consisting of just one linear layer. What accuracy can you achieve based on your representations? Compare against the accuracy of your supervised model. **(RESULT)**\n",
    "\n",
    "Hint: Keep in mind to quickly finetune the 1-layer projection head for testing the quality of your representations. It needs a small calibration to classify properly based on the provided embeddings of your Contrastive Learning model. You can use the same hyperparameters as for your supervised model in 15.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15.3: Contrastive Loss with Margin\n",
    "\n",
    "- Implement a contrastive loss function with margin. Does this improve your representation quality? Check the accuracy with a classifier just like in 15.2. \n",
    "- Compare your results with 15.1 and 15.2. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uptodate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
