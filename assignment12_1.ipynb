{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 12.1 - Recurrent Neural Networks\n",
    "\n",
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf.\n",
    "\n",
    "#### Please state both names of your group members here:\n",
    "Jane and John Doe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 12.1.1: RNN - 'ShakesGen'\n",
    "\n",
    "Let's create a `ShakesGen` !!<br><br>\n",
    "The data folder contains a shakespeare folder with works from William Shakespeare. Your task is to implement an RNN that learns to write Shakespeare-style text.\n",
    "\n",
    "Below, you'll find all the utility code needed for this task. The Corpus class serves as a dataset, and you can retrieve a batch with its target by calling `get_batch` on a batchified dataset.\n",
    "\n",
    "* Build the missing model components and train your ShakesGen model. **(RESULT)**\n",
    "* Generate at least 30 lines of text using your ShakesGen model. **(RESULT)**\n",
    "\n",
    "Especially, if you train on cpu, you can stop training after 5 minutes and generate based on the current model state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/4000/0*WdbXF_e8kZI1R5nQ.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        \n",
    "        # This is very english language specific\n",
    "        # We will ingest only these characters:\n",
    "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
    "        \n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids\n",
    "    \n",
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i, bptt_size=35):\n",
    "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Corpus to load data\n",
    "corpus = Corpus('./data/shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)\n",
    "\n",
    "# Print first 100 words from training data\n",
    "words = [corpus.dictionary.idx2word[corpus.train[i].item()] for i in range(min(100, len(corpus.train)))]\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = corpus.dictionary.word2idx.get(\"That\", -1)  # returns -1 if not found\n",
    "print(f\"Index of the word 'That': {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement RNN model class & Training loop here\n",
    "\n",
    "# Tip: Use an Embedding layer to Tokenize each word.\n",
    "# e.g., self.embedding = nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
